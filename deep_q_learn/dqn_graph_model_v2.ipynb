{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym.spaces\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "import logz\n",
    "import tensorflow                as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "from collections import namedtuple\n",
    "from dqn_utils import *\n",
    "import replay_buffer_graph\n",
    "import time\n",
    "\n",
    "OptimizerSpec = namedtuple(\"OptimizerSpec\", [\"constructor\", \"kwargs\", \"lr_schedule\"])\n",
    "\n",
    "\"\"\"\n",
    "learn slightly modified to pass the task name as an argument\n",
    "so that it is easier to record data.\n",
    "\"\"\"\n",
    "\n",
    "def learn(env,\n",
    "          q_func,\n",
    "          pre_pooling_mlp_layers,\n",
    "          post_pooling_mlp_layers,\n",
    "          n_hidden_units,\n",
    "          T=4,\n",
    "          initialization_stddev=1e-4,\n",
    "          exploration=LinearSchedule(1000000, 0.1),\n",
    "          stopping_criterion=None,\n",
    "          replay_buffer_size=1000000,\n",
    "          batch_size=32,\n",
    "          gamma=0.99,\n",
    "          learning_starts=50000,\n",
    "          learning_freq=4,\n",
    "          frame_history_len=4,\n",
    "          target_update_freq=10000,\n",
    "          grad_norm_clipping=10,\n",
    "          double_DQN=True,\n",
    "          n_steps_ahead=3,\n",
    "          learning_rate=1e-3\n",
    "         ):\n",
    "    \"\"\"Run Deep Q-learning algorithm.\n",
    "    You can specify your own convnet using q_func.\n",
    "    All schedules are w.r.t. total number of steps taken in the environment.\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.Env\n",
    "        gym environment to train on.\n",
    "    q_func: function\n",
    "        Model to use for computing the q function. It should accept the\n",
    "        following named arguments:\n",
    "            img_in: tf.Tensor\n",
    "                tensorflow tensor representing the input image\n",
    "            num_actions: int\n",
    "                number of actions\n",
    "            scope: str\n",
    "                scope in which all the model related variables\n",
    "                should be created\n",
    "            reuse: bool\n",
    "                whether previously created variables should be reused.\n",
    "    optimizer_spec: OptimizerSpec\n",
    "        Specifying the constructor and kwargs, as well as learning rate schedule\n",
    "        for the optimizer\n",
    "    session: tf.Session\n",
    "        tensorflow session to use.\n",
    "    exploration: rl_algs.deepq.utils.schedules.Schedule\n",
    "        schedule for probability of chosing random action.\n",
    "    stopping_criterion: (env, t) -> bool\n",
    "        should return true when it's ok for the RL algorithm to stop.\n",
    "        takes in env and the number of steps executed so far.\n",
    "    replay_buffer_size: int\n",
    "        How many memories to store in the replay buffer.\n",
    "    batch_size: int\n",
    "        How many transitions to sample each time experience is replayed.\n",
    "    gamma: float\n",
    "        Discount Factor\n",
    "    learning_starts: int\n",
    "        After how many environment steps to start replaying experiences\n",
    "    learning_freq: int\n",
    "        How many steps of environment to take between every experience replay\n",
    "    frame_history_len: int\n",
    "        How many past frames to include as input to the model.\n",
    "    target_update_freq: int\n",
    "        How many experience replay rounds (not steps!) to perform between\n",
    "        each update to the target Q network\n",
    "    grad_norm_clipping: float or None\n",
    "        If not None gradients' norms are clipped to this value.\n",
    "    \"\"\"\n",
    "    exp_name = env.env_name\n",
    "    logz.configure_output_dir('data/' + exp_name + time.strftime('%m-%d-%Y-%H:%M:%s'))\n",
    "    ###############\n",
    "    # BUILD MODEL #\n",
    "    ###############\n",
    "\n",
    "    input_shape = env.state_shape\n",
    "    num_actions = env.num_actions\n",
    "\n",
    "    # set up placeholders\n",
    "    # placeholder for current observation (or state)\n",
    "    obs_t_ph              = tf.placeholder(tf.float32, [None] + list(input_shape))\n",
    "    # placeholder for current action\n",
    "    act_t_ph              = tf.placeholder(tf.int32, [None], name='act_t_ph')\n",
    "    # placeholder for current reward\n",
    "    rew_t_ph              = tf.placeholder(tf.float32, [None])\n",
    "    # placeholder for next observation (or state)\n",
    "    obs_tp1_ph            = tf.placeholder(tf.float32, [None] + list(input_shape))\n",
    "    \n",
    "    # placeholder for end of episode mask\n",
    "    # this value is 1 if the next state corresponds to the end of an episode,\n",
    "    # in which case there is no Q-value at the next state; at the end of an\n",
    "    # episode, only the current state reward contributes to the target, not the\n",
    "    # next state Q-value (i.e. target is just rew_t_ph, not rew_t_ph + gamma * q_tp1)\n",
    "    done_mask_ph          = tf.placeholder(tf.float32, [None])\n",
    "    transition_length_ph = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "    # Graphs specific placeholder\n",
    "    adj_ph = tf.placeholder(tf.float32, [None, env.number_nodes, env.number_nodes],\n",
    "                            name='adj_ph')\n",
    "    graph_weights_ph = tf.placeholder(tf.float32,\n",
    "                                      [None, env.number_nodes, env.number_nodes],\n",
    "                                      name='graph_weights_ph')\n",
    "    # Q network\n",
    "    q_func_net = q_func(x=obs_t_ph, # q function returns some sort of equation\n",
    "                        adj=adj_ph,\n",
    "                        w=graph_weights_ph,\n",
    "                        p=n_hidden_units, T=T, initialization_stddev=initialization_stddev,\n",
    "                        scope=\"q_func\", reuse=False,\n",
    "                        pre_pooling_mlp_layers=pre_pooling_mlp_layers,\n",
    "                        post_pooling_mlp_layers=post_pooling_mlp_layers)\n",
    "    q_func_net_argmax_target = q_func(x=obs_tp1_ph, # q function returns some sort of equation\n",
    "                                      adj=adj_ph,\n",
    "                                      w=graph_weights_ph,\n",
    "                                      p=n_hidden_units, T=T, initialization_stddev=initialization_stddev,\n",
    "                                      scope=\"q_func\", reuse=False,\n",
    "                                      pre_pooling_mlp_layers=pre_pooling_mlp_layers,\n",
    "                                      post_pooling_mlp_layers=post_pooling_mlp_layers)\n",
    "    #target network\n",
    "    target_q_func_net = q_func(x=obs_tp1_ph, # q function returns some sort of equation\n",
    "                               adj=adj_ph, \n",
    "                               w=graph_weights_ph,\n",
    "                               p=n_hidden_units, T=T, initialization_stddev=initialization_stddev,\n",
    "                               scope=\"target_q_func\", reuse=False,\n",
    "                               pre_pooling_mlp_layers=pre_pooling_mlp_layers,\n",
    "                               post_pooling_mlp_layers=post_pooling_mlp_layers)\n",
    "\n",
    "    if not double_DQN:#deep q\n",
    "        target_y = rew_t_ph + tf.pow(gamma, transition_length_ph) *\\\n",
    "                              done_mask_ph * tf.reduce_max(target_q_func_net, axis=1)\n",
    "    else:#double deep q\n",
    "        target_y = rew_t_ph + \\\n",
    "                   tf.pow(gamma, transition_length_ph) * done_mask_ph * \\\n",
    "                   tf.reduce_sum(target_q_func_net *\\\n",
    "                                 tf.one_hot(tf.argmax(q_func_net_argmax_target, axis = 1),\n",
    "                                            depth=num_actions),\\\n",
    "                                 axis=1)\n",
    "    # (double) dqn mechanics\n",
    "    actual_y = tf.reduce_sum(tf.multiply(q_func_net, tf.one_hot(act_t_ph, depth=num_actions)), axis=1)\n",
    "    total_error = tf.nn.l2_loss(target_y - actual_y)\n",
    "    q_func_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                                    scope='q_func')\n",
    "    target_q_func_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                                           scope='target_q_func')\n",
    "\n",
    "    training_error_summ_sy = tf.summary.scalar('training_total_error', total_error)\n",
    "\n",
    "    # construct optimization op (with gradient clipping)\n",
    "    # learning_rate = tf.placeholder(tf.float32, (), name=\"learning_rate\")\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_fn = optimizer.minimize(total_error)\n",
    "\n",
    "    # update_target_fn will be called periodically to copy Q network to target Q network\n",
    "    update_target_fn = []\n",
    "    for var, var_target in zip(sorted(q_func_vars,        key=lambda v: v.name),\n",
    "                               sorted(target_q_func_vars, key=lambda v: v.name)):\n",
    "        update_target_fn.append(var_target.assign(var))\n",
    "    update_target_fn = tf.group(*update_target_fn)\n",
    "\n",
    "    # construct the replay buffer\n",
    "    replay_buffer = replay_buffer_graph.ReplayBuffer(replay_buffer_size, obs_size=input_shape[0],\n",
    "                                                     n_nodes=input_shape[0])\n",
    "\n",
    "    # Model saver\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # Create session, initialize variables\n",
    "    session = tf.InteractiveSession()\n",
    "    log_files_name = 'DQN_' + str(env.env_name) + \\\n",
    "                     '-lf=' + str(learning_freq) + \\\n",
    "                     '-b=' + str(batch_size) + '-' + \\\n",
    "                     time.strftime('%m-%d-%Y-%H:%M:%S')\n",
    "\n",
    "    writer = tf.summary.FileWriter('/tmp/' + log_files_name,\n",
    "                                   session.graph)\n",
    "    tf.global_variables_initializer().run()\n",
    "    saver.save(session, '/tmp/saved_models/' + log_files_name)\n",
    "\n",
    "    ###############\n",
    "    # RUN ENV     #\n",
    "    ###############\n",
    "    model_initialized = False\n",
    "    num_param_updates = 0\n",
    "    mean_episode_reward      = -float('nan')\n",
    "    best_mean_episode_reward = -float('inf')\n",
    "    observations = [env.reset()]\n",
    "    LOG_EVERY_N_STEPS = 10000\n",
    "\n",
    "    episode_total_rewards = []\n",
    "    episode_total_optimal_rewards = []\n",
    "    episode_total_at_random_rewards = []\n",
    "    accuracies = []\n",
    "    done = False\n",
    "    for t in itertools.count():\n",
    "        ### 1. Check stopping criterion\n",
    "        if stopping_criterion is not None and stopping_criterion(env, t):\n",
    "            break\n",
    "\n",
    "        ### 2. Step the env and store the transition\n",
    "        import random\n",
    "        from numpy import array\n",
    "\n",
    "        if done:\n",
    "            observations = [env.reset()]\n",
    "\n",
    "        # Choose action (epsilon greedy)\n",
    "        if model_initialized:\n",
    "            epsilon = exploration.value(t)\n",
    "            # learning a policy, q vals gives me that policy\n",
    "            q_values=session.run(q_func_net, feed_dict={obs_t_ph: observations[-1][None],\n",
    "                                                        adj_ph: env.adjacency_matrix[None],\n",
    "                                                        graph_weights_ph: env.weight_matrix[None]})\n",
    "\n",
    "            # using function to pick an action\n",
    "            action = np.argmax(q_values[0] * (1 - observations[-1]) - 1e5 * observations[-1])\n",
    "            r = random.random()\n",
    "            if r <= epsilon:\n",
    "                all_possible_action = list(range(num_actions))\n",
    "                # other_actions = [x for x in all_possible_action if x != action]\n",
    "                if env.env_name == 'MVC':\n",
    "                    other_actions = [x for x in all_possible_action if env.state[x] != 1]\n",
    "                else:\n",
    "                    other_actions = [x for x in all_possible_action if observations[-1][x] != 1]\n",
    "                # choose random action from among others\n",
    "                action = np.array(random.choice(other_actions))\n",
    "        else:\n",
    "            action = np.array(random.choice(list(range(num_actions))))\n",
    "        #forcast n-steps\n",
    "        next_obs, reward, done = env.step(action)\n",
    "        observations.append(next_obs)\n",
    "\n",
    "        if len(observations) > n_steps_ahead:\n",
    "            replay_buffer.store_transition(observations[-(n_steps_ahead + 1)], env.adjacency_matrix,\n",
    "                                           env.weight_matrix, action, reward, observations[-1], done, n_steps_ahead)\n",
    "\n",
    "        if done and 1 < len(observations) <= n_steps_ahead:\n",
    "            replay_buffer.store_transition(observations[0], env.adjacency_matrix, env.weight_matrix,\n",
    "                                           action, reward, observations[-1], done, len(observations) - 1)\n",
    "\n",
    "\n",
    "        # at this point, the environment should have been advanced one step (and\n",
    "        # reset if done was true), and last_obs should point to the new latest\n",
    "        # observation\n",
    "\n",
    "        ### 3. Perform experience replay and train the network.\n",
    "        # note that this is only done if the replay buffer contains enough samples\n",
    "        # for us to learn something useful -- until then, the model will not be\n",
    "        # initialized and random actions should be taken\n",
    "\n",
    "        if (t > learning_starts and\n",
    "                t % learning_freq == 0 and\n",
    "                replay_buffer.can_sample(batch_size)):\n",
    "\n",
    "            obs_t_batch, adj_batch, graph_weights_batch, act_batch,\\\n",
    "            rew_batch, obs_tp1_batch, done_mask_batch, transition_length_batch = replay_buffer.sample(batch_size)\n",
    "            if not(model_initialized):\n",
    "                initialize_interdependent_variables(session, tf.global_variables(), {\n",
    "                            obs_t_ph: obs_t_batch,\n",
    "                            obs_tp1_ph: obs_tp1_batch,\n",
    "                        })\n",
    "                model_initialized=True\n",
    "\n",
    "            training_error_summ, _ = session.run([training_error_summ_sy, train_fn],\n",
    "                                                 feed_dict={obs_t_ph: obs_t_batch,\n",
    "                                                            adj_ph: adj_batch,\n",
    "                                                            graph_weights_ph: graph_weights_batch,\n",
    "                                                            act_t_ph: act_batch,\n",
    "                                                            rew_t_ph:rew_batch,\n",
    "                                                            obs_tp1_ph: obs_tp1_batch,\n",
    "                                                            done_mask_ph: done_mask_batch,\n",
    "                                                            transition_length_ph: transition_length_batch})\n",
    "\n",
    "            # import pdb; pdb.set_trace()\n",
    "            if t % 100:\n",
    "                writer.add_summary(training_error_summ, t)\n",
    "                writer.flush()\n",
    "\n",
    "            if num_param_updates%target_update_freq == 0:\n",
    "                session.run(update_target_fn)\n",
    "            num_param_updates += 1\n",
    "\n",
    "            #####\n",
    "\n",
    "            ### 4. Log progress\n",
    "            # episode_rewards = get_wrapper_by_name(env, \"Monitor\").get_episode_rewards()\n",
    "            if done:\n",
    "                episode_total_rewards.append(env.accumulated_reward())\n",
    "                episode_total_optimal_rewards.append(env.optimal_solution()[0])\n",
    "                episode_total_at_random_rewards.append(env.at_random_solution()[0])\n",
    "            \n",
    "            mean_approx_ratio = 0\n",
    "            if len(episode_total_rewards) > 0:\n",
    "                mean_episode_reward = np.mean(np.array(episode_total_rewards)[-1000:])\n",
    "                mean_optimal_episode_reward = np.mean(np.array(episode_total_optimal_rewards)[-1000:])\n",
    "                mean_at_random_episode_reward = np.mean(np.array(episode_total_at_random_rewards)[-1000:])\n",
    "                if env.env_name == 'TSP':\n",
    "                    mean_approx_ratio = np.mean(np.array(episode_total_rewards)[-1000:] /\n",
    "                                                np.mean(np.array(episode_total_optimal_rewards)[-1000:]))\n",
    "\n",
    "            if len(episode_total_rewards) > 1000:\n",
    "                best_mean_episode_reward = max(best_mean_episode_reward, mean_episode_reward)\n",
    "\n",
    "            if t % LOG_EVERY_N_STEPS == 0 and model_initialized:\n",
    "                # Save the model\n",
    "                saver.save(session, '/tmp/saved_models/' + log_files_name, global_step=t)\n",
    "                # Display and log episode stats\n",
    "                logz.log_tabular(\"Timestep\", t)\n",
    "                logz.log_tabular(\"AtRandomAverageReturn\", mean_at_random_episode_reward)\n",
    "                logz.log_tabular(\"AverageReturn\", mean_episode_reward)\n",
    "                logz.log_tabular(\"OptimalAverageReturn\", mean_optimal_episode_reward)\n",
    "                if env.env_name == 'TSP':\n",
    "                    logz.log_tabular(\"ApproxRatio\", mean_approx_ratio)\n",
    "                logz.log_tabular(\"MaxReturn\", best_mean_episode_reward)\n",
    "                logz.log_tabular(\"Episodes\", len(episode_total_rewards))\n",
    "                logz.log_tabular(\"Exploration\", exploration.value(t))\n",
    "                logz.dump_tabular()\n",
    "\n",
    "                sys.stdout.flush()\n",
    "    return session\n",
    "# at this point we should have a saved model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(session, env, adjacency_matrix): # writen to look at a single test graph at a time...\n",
    "                        # currently a bunch of zeros\n",
    "    # placeholder for current observation\n",
    "    obs_t_ph              = tf.placeholder(tf.float32, [None] + list(input_shape))\n",
    "    # placeholder for current action\n",
    "    act_t_ph              = tf.placeholder(tf.int32, [None], name='act_t_ph')\n",
    "    # placeholder for current reward\n",
    "    rew_t_ph              = tf.placeholder(tf.float32, [None])\n",
    "    # placeholder for next observation (or state)\n",
    "    obs_tp1_ph            = tf.placeholder(tf.float32, [None] + list(input_shape))\n",
    "    \n",
    "    # placeholder for end of episode mask\n",
    "    # this value is 1 if the next state corresponds to the end of an episode,\n",
    "    # in which case there is no Q-value at the next state; at the end of an\n",
    "    # episode, only the current state reward contributes to the target, not the\n",
    "    # next state Q-value (i.e. target is just rew_t_ph, not rew_t_ph + gamma * q_tp1)\n",
    "    done_mask_ph          = tf.placeholder(tf.float32, [None])\n",
    "    transition_length_ph = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "    # Graphs specific placeholder\n",
    "    adj_ph = tf.placeholder(tf.float32, [None, env.number_nodes, env.number_nodes],\n",
    "                            name='adj_ph')\n",
    "    graph_weights_ph = tf.placeholder(tf.float32,\n",
    "                                      [None, env.number_nodes, env.number_nodes],\n",
    "                                      name='graph_weights_ph')\n",
    "    # Q network\n",
    "    q_func_net = q_func(x=obs_t_ph, # q function returns some sort of equation\n",
    "                        adj=adj_ph,\n",
    "                        w=graph_weights_ph,\n",
    "                        p=n_hidden_units, T=T, initialization_stddev=initialization_stddev,\n",
    "                        scope=\"q_func\", reuse=False,\n",
    "                        pre_pooling_mlp_layers=pre_pooling_mlp_layers,\n",
    "                        post_pooling_mlp_layers=post_pooling_mlp_layers)\n",
    "    #q_func_net_argmax_target = q_func(x=obs_tp1_ph, # q function returns some sort of equation\n",
    "                                      #adj=adj_ph,\n",
    "                                      #w=graph_weights_ph,\n",
    "                                      #p=n_hidden_units, T=T, initialization_stddev=initialization_stddev,\n",
    "                                      #scope=\"q_func\", reuse=False,\n",
    "                                      #pre_pooling_mlp_layers=pre_pooling_mlp_layers,\n",
    "                                      #post_pooling_mlp_layers=post_pooling_mlp_layers)\n",
    "    #target network\n",
    "    #target_q_func_net = q_func(x=obs_tp1_ph, # q function returns some sort of equation\n",
    "                               #adj=adj_ph, \n",
    "                               #w=graph_weights_ph,\n",
    "                               #p=n_hidden_units, T=T, initialization_stddev=initialization_stddev,\n",
    "                               #scope=\"target_q_func\", reuse=False,\n",
    "                               #pre_pooling_mlp_layers=pre_pooling_mlp_layers,\n",
    "                               #post_pooling_mlp_layers=post_pooling_mlp_layers)\n",
    "\n",
    "    #if not double_DQN:#deep q\n",
    "        #target_y = rew_t_ph + tf.pow(gamma, transition_length_ph) *\\\n",
    "                              #done_mask_ph * tf.reduce_max(target_q_func_net, axis=1)\n",
    "    #else:#double deep q\n",
    "        #target_y = rew_t_ph + \\\n",
    "                   #tf.pow(gamma, transition_length_ph) * done_mask_ph * \\\n",
    "                   #tf.reduce_sum(target_q_func_net *\\\n",
    "                                 #tf.one_hot(tf.argmax(q_func_net_argmax_target, axis = 1),\n",
    "                                            #depth=num_actions),\\\n",
    "                                 #axis=1)\n",
    "    # (double) dqn mechanics\n",
    "    #actual_y = tf.reduce_sum(tf.multiply(q_func_net, tf.one_hot(act_t_ph, depth=num_actions)), axis=1)\n",
    "    #total_error = tf.nn.l2_loss(target_y - actual_y)\n",
    "    #q_func_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                                    #scope='q_func')\n",
    "    #target_q_func_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                                           #scope='target_q_func')\n",
    "\n",
    "    #training_error_summ_sy = tf.summary.scalar('training_total_error', total_error)\n",
    "\n",
    "    # construct optimization op (with gradient clipping)\n",
    "    # learning_rate = tf.placeholder(tf.float32, (), name=\"learning_rate\")\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    #train_fn = optimizer.minimize(total_error)\n",
    "\n",
    "    # update_target_fn will be called periodically to copy Q network to target Q network\n",
    "    #update_target_fn = []\n",
    "    #for var, var_target in zip(sorted(q_func_vars,        key=lambda v: v.name),\n",
    "                               #sorted(target_q_func_vars, key=lambda v: v.name)):\n",
    "        #update_target_fn.append(var_target.assign(var))\n",
    "    #update_target_fn = tf.group(*update_target_fn)\n",
    "\n",
    "    # construct the replay buffer\n",
    "    replay_buffer = replay_buffer_graph.ReplayBuffer(replay_buffer_size, obs_size=input_shape[0],\n",
    "                                                     n_nodes=input_shape[0])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # we have a saved model at this point.\n",
    "    # We would call the saved model in main, and run test using that trained model and some input data\n",
    "    done = False \n",
    "    # over time, we're waiting until this thing is done\n",
    "    for t in itertools.count():\n",
    "        ### 1. Check stopping criterion (once we finish with the graph, we are finished with the graph)\n",
    "        model_initialized = True\n",
    "        if done == True:\n",
    "            break\n",
    "        if model_initialized:\n",
    "            #epsilon = exploration.value(t)/10\n",
    "            epsilon = .05\n",
    "\n",
    "            # using function to pick an action, guided by our existing set of Q-values\n",
    "            action = np.argmax(q_values[0] * (1 - observations[-1]) - 1e5 * observations[-1])\n",
    "            r = random.random()\n",
    "            if r <= epsilon:\n",
    "                all_possible_action = list(range(num_actions))\n",
    "                other_actions = [x for x in all_possible_action if observations[-1][x] != 1]\n",
    "                #choose random action from among others\n",
    "                action = np.array(random.choice(other_actions))\n",
    "        else:\n",
    "            action = np.array(random.choice(list(range(num_actions))))\n",
    "            #forcast n-steps\n",
    "        next_obs, reward, done = env.step(action)\n",
    "        observations.append(next_obs)\n",
    "\n",
    "       \n",
    "        #####\n",
    "\n",
    "        ### 4. Log progress\n",
    "        # episode_rewards = get_wrapper_by_name(env, \"Monitor\").get_episode_rewards()\n",
    "        mean_approx_ratio = 'not computed yet'\n",
    "        if done:\n",
    "            episode_total_rewards.append(env.accumulated_reward())\n",
    "            episode_total_optimal_rewards.append(env.optimal_solution()[0])\n",
    "            episode_total_at_random_rewards.append(env.at_random_solution()[0])\n",
    "            if env.env_name =='TSP':\n",
    "                mean_approx_ratio = np.mean(np.array(episode_total_rewards) /\n",
    "                                                np.mean(np.array(episode_total_optimal_rewards)))\n",
    "\n",
    "        if len(episode_total_rewards) > 0:\n",
    "            mean_episode_reward = np.mean(np.array(episode_total_rewards)[-1000:])\n",
    "            mean_optimal_episode_reward = np.mean(np.array(episode_total_optimal_rewards)[-1000:])\n",
    "            mean_at_random_episode_reward = np.mean(np.array(episode_total_at_random_rewards)[-1000:])\n",
    "            if env.env_name == 'TSP':\n",
    "                mean_approx_ratio = np.mean(np.array(episode_total_rewards)[-1000:] /\n",
    "                                                np.mean(np.array(episode_total_optimal_rewards)[-1000:]))\n",
    "\n",
    "        if len(episode_total_rewards) > 1000:\n",
    "            best_mean_episode_reward = max(best_mean_episode_reward, mean_episode_reward)\n",
    "\n",
    "        if t % LOG_EVERY_N_STEPS == 0 and model_initialized:\n",
    "            # Save the model\n",
    "            #saver.save(session, '/tmp/saved_models/' + log_files_name, global_step=t)\n",
    "            # Display and log episode stats\n",
    "            logz.log_tabular(\"Timestep\", t)\n",
    "            logz.log_tabular(\"AtRandomAverageReturn\", mean_at_random_episode_reward)\n",
    "            logz.log_tabular(\"AverageReturn\", mean_episode_reward)\n",
    "            logz.log_tabular(\"OptimalAverageReturn\", mean_optimal_episode_reward)\n",
    "            if env.env_name == 'TSP':\n",
    "                logz.log_tabular(\"ApproxRatio\", mean_approx_ratio)\n",
    "            logz.log_tabular(\"MaxReturn\", best_mean_episode_reward)\n",
    "            logz.log_tabular(\"Episodes\", len(episode_total_rewards))\n",
    "            logz.log_tabular(\"Exploration\", exploration.value(t))\n",
    "            logz.dump_tabular()\n",
    "\n",
    "            sys.stdout.flush()\n",
    "    return observations, mean_approx_ratio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
